{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Pegando todas as campanhas e seus links\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# API das campanhas\n",
    "api_url = 'https://dashboard.wikiedu.org/lookups/campaign.json'\n",
    "response = requests.get(api_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    campanhas = []\n",
    "\n",
    "    for program in data['campaigns']:\n",
    "        slug = program['slug']\n",
    "        campanhas.append([slug])\n",
    "    df_camp = pd.DataFrame(campanhas, columns=['Links'])\n",
    "\n",
    "    # Gerando os links completos de cada campanha\n",
    "    base_url = \"https://dashboard.wikiedu.org/campaigns/\"\n",
    "    df_camp['Links'] = df_camp['Links'].apply(lambda row: f'{base_url}{row}')\n",
    "\n",
    "# Filtrando programas de antes de 2015\n",
    "def filtro_ano(name):\n",
    "    match = re.search(r'\\b\\d{4}\\b', name)\n",
    "    year = int(match.group()) if match else 0\n",
    "    return year >= 2015 or not match\n",
    "\n",
    "# Filtrar linhas com base no ano\n",
    "df_camp = df_camp[df_camp['Links'].apply(filtro_ano)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Pegando os links dos cursos de cada campanha\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def extrair_links_cursos(url):\n",
    "    links_cursos = set()\n",
    "\n",
    "    while url:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            course_elements = soup.find_all('a', class_='course-link')\n",
    "            for course_element in course_elements:\n",
    "                link = urljoin(url, course_element['href'])  # Para adicionar a terminação ao link completo\n",
    "                links_cursos.add(link)\n",
    "\n",
    "            # Verificando se há outras páginas\n",
    "            pagination_div = soup.find('div', class_='pagination')\n",
    "            if pagination_div:\n",
    "                next_page = pagination_div.find('a', class_='next_page')\n",
    "                if next_page:\n",
    "                    url = urljoin(url, next_page['href'])\n",
    "                else:\n",
    "                    url = None\n",
    "            else:\n",
    "                url = None\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    return list(links_cursos)\n",
    "\n",
    "links_campanhas = df_camp['Links']\n",
    "\n",
    "# Tentativa de acelerar a extração\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    curso_links = []\n",
    "    for link in links_campanhas:\n",
    "        curso_links.extend(extrair_links_cursos(link))\n",
    "\n",
    "curso_links = list(set(curso_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Extraindo as informações requisitadas\n",
    "\n",
    "# Separado em 3 funções por serem 3 arquivos JSON diferentes\n",
    "\n",
    "# Nome da campanha\n",
    "def buscar_campanha(link):\n",
    "    api_campanha = f\"{link}/campaigns.json\"\n",
    "    response_campanha = requests.get(api_campanha)\n",
    "    \n",
    "    if response_campanha.status_code == 200:\n",
    "        campanha_data = response_campanha.json()\n",
    "        if 'course' in campanha_data and 'campaigns' in campanha_data['course']:\n",
    "            campaigns = campanha_data['course']['campaigns']\n",
    "            if campaigns:\n",
    "                return {'Campanha': campaigns[0]['title']}  # Retorna o título da primeira campanha\n",
    "    return None\n",
    "\n",
    "# Informações dos cursos\n",
    "def buscar_curso(link):\n",
    "    api_curso = f\"{link}/course.json\"\n",
    "    response_curso = requests.get(api_curso)\n",
    "    \n",
    "    if response_curso.status_code == 200:\n",
    "        curso_data = response_curso.json()['course']\n",
    "        return {\n",
    "            'Curso': curso_data['title'],\n",
    "            'Descrição': curso_data['description'],\n",
    "            'Início': curso_data['start'],\n",
    "            'Encerramento': curso_data['end'],\n",
    "            'Instituição': curso_data['school'],\n",
    "            'Link': link\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def buscar_usuarios(link):\n",
    "    api_users = f\"{link}/users.json\"\n",
    "    response_users = requests.get(api_users)\n",
    "    \n",
    "    if response_users.status_code == 200:\n",
    "        users_data = response_users.json()\n",
    "        instructors = []\n",
    "        wiki_ed_staff = []\n",
    "        \n",
    "        for user in users_data['course']['users']:\n",
    "            if 'real_name' in user:  # Verifique se 'real_name' está presente\n",
    "                if user['role'] == 1:\n",
    "                    if user['real_name'] is None or user['real_name'].lower() == \"null\":\n",
    "                        instructors.append(user['username'])\n",
    "                    else:\n",
    "                        instructors.append(user['real_name'])\n",
    "                elif user['role'] == 4:\n",
    "                    if user['real_name'] is None or user['real_name'].lower() == \"null\":\n",
    "                        wiki_ed_staff.append(user['username'])\n",
    "                    else:\n",
    "                        wiki_ed_staff.append(user['real_name'])\n",
    "        \n",
    "        return {'Facilitadores': instructors, 'Wiki Ed Staff': wiki_ed_staff}\n",
    "    \n",
    "    return None\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    campanhas = list(executor.map(buscar_campanha, curso_links))\n",
    "    cursos = list(executor.map(buscar_curso, curso_links))\n",
    "    usuarios = list(executor.map(buscar_usuarios, curso_links))\n",
    "\n",
    "# Criando um dicionário para cada curso\n",
    "info_cursos = []\n",
    "for link, campanha, curso, usuario in zip(curso_links, campanhas, cursos, usuarios):\n",
    "    curso_info = {'Link': link}\n",
    "    if campanha:\n",
    "        curso_info.update(campanha)\n",
    "    info_cursos.append(curso_info)\n",
    "    if curso:\n",
    "        curso_info.update(curso)\n",
    "    info_cursos.append(curso_info)\n",
    "    if usuario:\n",
    "        curso_info.update(usuario)\n",
    "    info_cursos.append(curso_info)\n",
    "\n",
    "df = pd.DataFrame(info_cursos)\n",
    "\n",
    "# Salvando em csv\n",
    "df.to_csv('Dados.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Tratando o df\n",
    "\n",
    "# Removando caracteres não aceitos pelo excel\n",
    "df = df.applymap(lambda x: x.replace('\\u000b', '') if isinstance(x, str) else x)\n",
    "\n",
    "# Formatando as datas e ordenando dos mais recentes para mais antigos\n",
    "df['Encerramento'] = pd.to_datetime(df['Encerramento'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce')\n",
    "df = df[df['Encerramento'].dt.year >= 2015]\n",
    "df['Início'] = pd.to_datetime(df['Início'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce')\n",
    "df = df.dropna(subset=['Encerramento'])\n",
    "df = df.sort_values(by='Encerramento', ascending=False)\n",
    "df['Encerramento'] = df['Encerramento'].dt.strftime('%d/%m/%Y')\n",
    "df['Início'] = df['Início'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Corrigindo os nomes\n",
    "def converter_lista_para_str(valor):\n",
    "    if isinstance(valor, list):\n",
    "        return ', '.join(map(str, valor)) \n",
    "    return valor\n",
    "\n",
    "df = df.applymap(converter_lista_para_str)\n",
    "\n",
    "# Tirando duplicatas\n",
    "df = df.drop_duplicates(subset=['Link'])\n",
    "\n",
    "# Reordenando as colunas\n",
    "ordem = ['Campanha', 'Curso', 'Descrição', 'Instituição','Início', 'Encerramento', 'Facilitadores', 'Wiki Ed Staff', 'Link']\n",
    "df = df[ordem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Criando 2 arquivos em excel\n",
    "\n",
    "df2 = df.head(700)\n",
    "df.to_excel('Todos os cursos.xlsx', index=False)\n",
    "df2.to_excel('700 cursos.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
